---
title: 随机过程中的一些概念的理解
math: true
mermaid: false
date: 2024-10-01 10:57:53
tags:
    - stochastic processes
categories:
    - Math
---

## $\sigma$ 代数

> 直观理解：$\sigma$ 代数可以编码事件信息。

首先回顾 $\sigma$ 代数的定义：

> 对于样本点空间 $\Omega$，如果 $\mathcal{F} \subseteq 2^\Omega$ 满足
> - $\emptyset, \Omega \in \mathcal{F}$
> - $\forall A \in \mathcal{F}, A^c \in \mathcal{F}$
> - $\forall A_1, A_2, \ldots, A_n: \cup_{i=1}^n A_i \in \mathcal{F}$，这里 $n$ 可以是 $\infty$
>
> 则称 $\mathcal{F}$ 为 $\Omega$ 上的一个 $\sigma$ 代数

可以这样理解：

- $\sigma$ 代数中的每个集合对应于一个事件（即可能发生的情形）。$\sigma$ 代数中的元素越多，我们所能描述的事件越多，也就是说，我们所知道的信息越多。
- 如果某个 $\sigma$ 代数表示了我们在某一时刻掌握的信息，那么任何这个信息的合理组合（如补集或并集）也应该是我们已知的。因此，$\sigma$ 代数的封闭性很好地对应了信息在数学上的处理方式。

也就是说，一个 $\sigma$ 代数实际上是我们已知的信息（事件）的总和，并且它遵循了一些我们认为合理的结构（例如知道一件事，就能知道它的反面；知道若干事，就能知道它们综合起来的信息）。

## 随机变量关于 $\sigma$ 代数可测

先陈述一下这个标题：随机变量 $Z$ 关于一个 $\sigma$ 代数 $\mathcal{F}$ 是可测的，也就是说，$\forall z \in \mathbb{R}$，事件 $\{Z \le z\} \in \mathcal{F}$。

通俗的话讲，$\sigma$ 代数表示了我们已知的所有信息，“可测”意味着 $Z$ 的值可以仅通过这些信息来计算或推断（亦即，可以完全知道 $Z$ 的分布情况）。

## 为什么当条件为 $\sigma$ 代数时，条件期望可以是一个随机变量

> 对随机变量 $X$ 和 $\sigma$ 代数 $\mathcal{F}$，条件期望 $E[X|\mathcal{F}]$ 是一个随机变量。

可以这样理解：

对任意一个随机变量 $X$，可以看作是我们预先知道了一些信息得来的，但这信息有可能不全（例如当对应的 $\sigma$ 代数 $\mathcal{F}_0 \neq 2^\Omega$ 的时候）。这样 $E[X|\mathcal{F}]$ 就是给 $X$ 提供额外的信息，当总的信息不够多时，当然会得到一个新的随机变量。

当 $X$ 恰好可以由 $\mathcal{F}$ 的信息来推断时（即 $X$ is measurable with $\mathcal{F}$），再给出 $\mathcal{F}$ 中相同的信息并不能对 $X$ 做出新的限制/提供新的信息，所以有 $E[X\mid \mathcal{F}] = X$。

当总的信息够多时，$X$ 可以被完全的确定（例如 $\mathcal{F} = 2^\Omega$ 时，相当于告诉了全部信息），这个时候，条件期望就是一个确定的值了。

## 鞅（Martingale）

直观来说，鞅就是一个公平游戏的过程：

- 本次游戏的结果完全依赖于前面所有次游戏的结果（这和 Markov 过程不一样，但共同点是只依赖先前的信息）
- 公平性：本次游戏过后，统计角度来看，收益是不会变化的，也就是 $E[Z_n \mid X_1, \ldots, X_{n-1}] = Z_{n-1}$；或者以 $\sigma$ 代数的形式：$E[Z_n \mid \mathcal{F}\_{n-1}] = Z_{n-1}$

## 过滤（filtration）

在鞅的广义定义中，用 $\mathcal{F_n}$ 代替了 $\{X_1, X_2, \ldots, X_n\}$，这是因为

$$
E[Z_{n+1} \mid X_1, X_2, \ldots, X_n] = E[Z_{n+1} \mid \sigma (\{X_1, X_2, \ldots, X_n\})]
$$

实际上就是，用更一般的 $\sigma$ 代数 $\mathcal{F_n}$ 来代替 $\sigma (\{X_1, X_2, \ldots, X_n\})$，而非形式上的表象 $\{X_1, X_2, \ldots, X_n\}$。

又因为

$$
\sigma (\{X_1, X_2, \ldots, X_n\}) \subseteq \sigma (\{X_1, X_2, \ldots, X_n, X_{n+1}\})
$$

所以自然也需要规定

$$
\mathcal{F}\_{n} \subseteq \mathcal{F}_{n+1}
$$

这样看来，随着时间的推移（或游戏的进行），我们获得的信息就越来越多（$\mathcal{F}_1 \subseteq \cdots \mathcal{F}_n \subseteq \cdots$），也就可以形象化地理解为“**过滤**”操作了。

## Doob Martingale

先摆出公式：

$$
Z_i = E\left[f(X_1, \ldots, X_n) \mid X_1, \ldots, X_i \right]
$$

有了以上关于鞅和过滤的理解，Doob Martingale 就很好理解了：

- Doob Martingale 就是一个逐步增加信息的过程：从信息为零到最后完全知道信息
- 刚开始 $Z_0$ 时，关于 $\{ X_1, \ldots, X_n\}$ 的一点信息都不知道，所以自然有 $Z_0 = E[f(X_1, \ldots, X_n)]$
- 随着信息逐渐增多，$\{ Z_i\}_{0 \le i \le n}$ 自然构成一个鞅
- 最后信息完全得知，便有 $Z_n = f(X_1, \ldots, X_n)$

## Azuma-Hoeffding’s Inequality

有了以上关于鞅和过滤的理解，Azuma-Hoeffding’s Inequality 也很好理解了：

- $\mathrm{Pr}[|Z_i - Z_{i-1}| \le c_i] = 1$：表示时间 $i$ 处获得的信息差几乎一定小于 $c_i$
- 当任意一个时刻获得的信息差 $c_i$ 变大时，关于最初的随机变量 $Z_0$ 和最后的随机变量 $Z_n$ 之间的差距的可能上限就会变大
